{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.accuracy_metrics import AccuracyMetrics\n",
    "from metrics.similarity_metrics import CosineSimilarity\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loaders.elco_dataloader import get_loaders\n",
    "train_loader, test_loader = get_loaders(\"data/ELCo.csv\", batch_size=1, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "class TeacherModel():\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.model.encode(x, convert_to_tensor=True)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.encode(x)\n",
    "\n",
    "class StudentModel():\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer('xlm-roberta-base')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model.encode(x, convert_to_tensor=True, )\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from itertools import chain\n",
    "\n",
    "class ELCoMTrainer:\n",
    "    def __init__(self, teacher_model, student_model, optim, lr, accuracy_metric):\n",
    "        self.teacher = teacher_model\n",
    "        self.student = student_model\n",
    "        # self.teacher_optimizer = optim(self.teacher.model._first_module().parameters(), lr=lr)\n",
    "        self.student_optimizer = optim(self.student.model._first_module().parameters(), lr=lr)\n",
    "        self.accuracy_metric = accuracy_metric\n",
    "        self.optim = optim\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.cosine = nn.CosineSimilarity()\n",
    "        self.similarity = CosineSimilarity()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def train(self, train_loader, epochs):\n",
    "        # self.teacher.train()\n",
    "        # self.teacher.to(self.device)\n",
    "        # self.student.train()\n",
    "        # self.student.to(self.device)\n",
    "        total_loss = 0\n",
    "        for epoch in range(epochs):\n",
    "            for x, y in tqdm(train_loader):\n",
    "                teacher_en = Variable(self.teacher(x), requires_grad=True).mean(axis=0).unsqueeze(0)\n",
    "                student_en = Variable(self.student(x), requires_grad=True).mean(axis=0).unsqueeze(0)\n",
    "                student_em = Variable(self.student(y), requires_grad=True)\n",
    "                loss = self.mse(student_en, teacher_en) + self.mse(student_em, teacher_en)\n",
    "                + self.cosine(student_en, student_em).item()\n",
    "                # self.teacher_optimizer.zero_grad()\n",
    "                self.student_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # self.teacher_optimizer.step()\n",
    "                self.student_optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            print(f\"Epoch {epoch + 1} loss: {total_loss / len(train_loader)}\")\n",
    "        print(\"Training finished\")\n",
    "    \n",
    "    def test(self, test_loader):\n",
    "        \n",
    "        accuracy = 0\n",
    "        for x, y in tqdm(test_loader):\n",
    "            y = y[0]\n",
    "            rank_scores = []\n",
    "            y_score = self.student(y)\n",
    "            for rank, sentences in x.items():\n",
    "                sentences = list(chain(*sentences))\n",
    "                embeddings = self.student(sentences).mean(axis=0) # simple mean\n",
    "                rank_scores.append((self.similarity(embeddings, y_score).cpu().detach().numpy()[0], rank))\n",
    "            sorted_rank_scores = sorted(rank_scores, key=lambda x: x[0], reverse=True)\n",
    "            accuracy += self.accuracy_metric(sorted_rank_scores)\n",
    "        return accuracy / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name C:\\Users\\Shirshajit/.cache\\torch\\sentence_transformers\\xlm-roberta-base. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at C:\\Users\\Shirshajit/.cache\\torch\\sentence_transformers\\xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "teacher_model = TeacherModel()\n",
    "student_model = StudentModel()\n",
    "lr = 1e-3\n",
    "trainer = ELCoMTrainer(teacher_model, student_model, Adam, lr, AccuracyMetrics().top_k_accuracy)\n",
    "# trainer.train(train_loader, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AccuracyMetrics:\n",
    "    def __init__(self, k=5):\n",
    "        self.metrics = {}\n",
    "        self.k = k\n",
    "\n",
    "    \"\"\"\n",
    "    List of metrics:\n",
    "    - Accuracy\n",
    "    - Precision/Recall and F1 + Confusion Matrix\n",
    "    - ROC/AUC Curve\n",
    "    - Mean Reciprocal Rank (MRR)\n",
    "    - Precision@k\n",
    "    - Hit Rate\n",
    "    \"\"\"\n",
    "\n",
    "    def accuracy(self, rank_scores):\n",
    "        correct = 0\n",
    "        for i in range(len(rank_scores)):\n",
    "            if rank_scores[i][1] == i:\n",
    "                correct += 1\n",
    "        self.metrics['accuracy'] = correct / len(rank_scores)\n",
    "        return self.metrics['accuracy']\n",
    "    \n",
    "    def top_k_accuracy(self, rank_scores):\n",
    "        correct = 0\n",
    "        k = self.k\n",
    "        # if the max rank score is in the top k, then it is correct\n",
    "        if 0 in [rank_scores[i][1] for i in range(k)]:\n",
    "            correct += 1    \n",
    "        else:\n",
    "            print(rank_scores)\n",
    "        self.metrics['top_k_accuracy'] = correct\n",
    "        return self.metrics['top_k_accuracy']\n",
    "\n",
    "    def precision_recall_f1(self, rank_scores, y):\n",
    "        # TODO: Add functionality\n",
    "        self.metrics['precision'] = None\n",
    "        self.metrics['recall'] = None\n",
    "        self.metrics['f1'] = None\n",
    "        return self.metrics['precision'], self.metrics['recall'], self.metrics['f1']\n",
    "\n",
    "    def roc_auc(self, rank_scores, y):\n",
    "        # TODO: Add functionality\n",
    "        self.metrics['roc_auc'] = None\n",
    "        return self.metrics['roc_auc']\n",
    "\n",
    "    def mrr(self, rank_scores, y):\n",
    "        # TODO: Add functionality\n",
    "        self.metrics['mrr'] = None\n",
    "        return self.metrics['mrr']\n",
    "\n",
    "    def precision_at_k(self, rank_scores, y):\n",
    "        # TODO: Add functionality\n",
    "        self.metrics['precision_at_k'] = None\n",
    "        return self.metrics['precision_at_k']\n",
    "    \n",
    "    def hit_rate(self, rank_scores, y):\n",
    "        # TODO: Add functionality\n",
    "        self.metrics['hit_rate'] = None\n",
    "        return self.metrics['hit_rate']\n",
    "\n",
    "    def confusion_matrix(self, rank_scores, y):\n",
    "        # TODO: Add functionality\n",
    "        self.metrics['confusion_matrix'] = None\n",
    "        return self.metrics['confusion_matrix']\n",
    "\n",
    "    def plot_roc_auc(self, rank_scores, y):\n",
    "        # TODO: Add plot\n",
    "        pass\n",
    "\n",
    "    def plot_confusion_matrix(self, rank_scores, y):\n",
    "        # TODO: Add plot\n",
    "        pass\n",
    "\n",
    "    # Helper methods for metrics\n",
    "    def get(self, name):\n",
    "        return self.metrics[name]\n",
    "\n",
    "    def get_all(self):\n",
    "        return self.metrics\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.metrics)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.metrics)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [03:31<00:00,  5.03s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.16052532123960694"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = ELCoMTrainer(trainer.teacher, trainer.student, Adam, lr, AccuracyMetrics().accuracy)\n",
    "trainer.test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/42 [00:12<08:52, 12.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9922796, 3), (0.9904179, 5), (0.9883725, 2), (0.98767537, 7), (0.9876372, 4), (0.98716235, 6), (0.9867712, 0), (0.98652285, 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2/42 [00:19<06:02,  9.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.99708664, 2), (0.99395066, 1), (0.9934078, 7), (0.99210835, 6), (0.99186295, 5), (0.99183077, 3), (0.99168026, 0), (0.98931265, 4)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3/42 [00:26<05:24,  8.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.99513686, 3), (0.99513036, 1), (0.9941717, 5), (0.99331677, 4), (0.99235207, 2), (0.9911839, 0), (0.98942757, 6)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 8/42 [01:22<05:38,  9.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.993364, 4), (0.9925506, 5), (0.99181455, 6), (0.99124706, 7), (0.9910674, 3), (0.99018216, 2), (0.9897621, 0), (0.98676693, 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 11/42 [01:48<04:57,  9.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.99566334, 5), (0.9936409, 3), (0.99351627, 2), (0.99329317, 6), (0.9931414, 7), (0.99244803, 1), (0.991856, 0), (0.9887193, 4)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 12/42 [02:05<05:51, 11.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9945805, 4), (0.99353296, 3), (0.9935099, 1), (0.9931449, 5), (0.99299675, 2), (0.991795, 6), (0.99168444, 0), (0.99102175, 7)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 13/42 [02:12<04:55, 10.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.99487, 1), (0.99487, 4), (0.9942554, 3), (0.9941044, 5), (0.9932482, 2), (0.99180984, 0), (0.990287, 6)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 14/42 [02:22<04:47, 10.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9949983, 5), (0.9948519, 7), (0.99388844, 1), (0.99317145, 3), (0.9914884, 4), (0.99012774, 2), (0.9897674, 0), (0.9889223, 6)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 15/42 [02:29<04:09,  9.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9939599, 1), (0.9912823, 6), (0.99116606, 7), (0.9904037, 4), (0.99009246, 2), (0.9885861, 0), (0.98723, 5), (0.98566747, 3)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 17/42 [03:11<06:07, 14.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9959698, 4), (0.9957489, 1), (0.99561065, 2), (0.9949715, 3), (0.99428093, 5), (0.99390644, 6), (0.9925594, 7), (0.9922165, 0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 18/42 [03:15<04:38, 11.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.99590987, 2), (0.99590045, 4), (0.99590045, 7), (0.99584424, 6), (0.99524206, 5), (0.9950286, 3), (0.991818, 0), (0.99035674, 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 24/42 [04:47<03:48, 12.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.99679416, 3), (0.9966049, 1), (0.9938611, 6), (0.9921573, 7), (0.9920352, 5), (0.99186695, 4), (0.9912572, 0), (0.99077845, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 26/42 [05:03<02:42, 10.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.99551004, 5), (0.99238306, 6), (0.9915354, 4), (0.9913707, 1), (0.9911169, 3), (0.99032307, 0), (0.9898963, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 27/42 [05:11<02:21,  9.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9956853, 8), (0.99390376, 6), (0.9938203, 1), (0.9937954, 3), (0.9931139, 4), (0.9914742, 0), (0.98963654, 7), (0.98926586, 5), (0.9888343, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 28/42 [05:16<01:52,  8.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.99814683, 1), (0.9978796, 3), (0.99658734, 4), (0.9965856, 6), (0.9965856, 7), (0.9962928, 0), (0.99601245, 5), (0.9955719, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 29/42 [05:27<01:56,  8.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9960184, 2), (0.99594176, 4), (0.9956916, 5), (0.99564385, 3), (0.9950621, 7), (0.9938423, 1), (0.99239606, 6), (0.9917474, 0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 30/42 [05:32<01:32,  7.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9969924, 1), (0.9958421, 4), (0.9954656, 5), (0.99476504, 7), (0.9945866, 3), (0.9945175, 6), (0.99413663, 0), (0.99262714, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 32/42 [05:47<01:15,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9961684, 1), (0.99510026, 2), (0.99510026, 6), (0.9950396, 4), (0.9943682, 3), (0.99410516, 5), (0.99081147, 0), (0.99079764, 7)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 34/42 [06:04<01:01,  7.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9966995, 4), (0.99655324, 7), (0.9963093, 1), (0.9962782, 6), (0.99614483, 2), (0.9957789, 3), (0.9957364, 0), (0.99567735, 5)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 37/42 [06:36<00:43,  8.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9974639, 7), (0.99740857, 1), (0.9969399, 3), (0.99642503, 4), (0.99536353, 2), (0.99419457, 6), (0.993499, 0), (0.993499, 5)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 38/42 [06:46<00:36,  9.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.996256, 5), (0.994724, 4), (0.99377245, 2), (0.99347925, 3), (0.99321663, 6), (0.99314266, 1), (0.9913159, 7), (0.9898502, 0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 40/42 [07:02<00:17,  8.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9960773, 5), (0.99567574, 3), (0.99559814, 2), (0.995588, 1), (0.9952556, 7), (0.99512124, 4), (0.99483, 0), (0.9940717, 8), (0.9913796, 6)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [07:16<00:00, 10.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.99667317, 4), (0.99476063, 2), (0.9936729, 3), (0.9931971, 7), (0.99262595, 6), (0.9909542, 1), (0.9909542, 5), (0.9876985, 0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4523809523809524"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = ELCoMTrainer(trainer.teacher, trainer.student, Adam, lr, AccuracyMetrics().top_k_accuracy)\n",
    "trainer.test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out, _ = self.lstm(out)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out, _ = self.gru(out)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out, _ = self.rnn(out)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "    \n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
